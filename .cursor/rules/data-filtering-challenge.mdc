---
description: 
globs: 
alwaysApply: true
---
# ELMB Data Filtering Challenge - Complete Workflow Guide

## Contest Background
1. **Contest Goal**: Create data filtering techniques and submit refined datasets to significantly improve the performance of edge LMs on the ELMB benchmark through DoRA fine-tuning.
2. **Evaluation Benchmark**: ELMB (Edge Language Model Benchmark) includes 4 tasks: Roleplay, Reasoning, Function Calling, RAG.
3. **Winning Criteria**: Maximize S = Simprove - Sbase, where:
   - Sbase: Performance of the base model on ELMB (provided by the organizers)
   - Simprove: Performance of the model after DoRA fine-tuning with the filtered dataset.
4. **Data Limit**: Filtered dataset does not exceed 10B tokens.

## Complete Workflow Overview

### Phase 1: Data Filtering (Self-Developed)
```bash
# Step 1: Create test dataset (Verified)
python create_test_dataset.py
# Output: data/test_dataset/test_dataset.json (220 samples for testing)

# Step 2: Large-scale data filtering (To be developed)
# TBD: Implement a real 10B token data filtering pipeline
# python filter_large_dataset.py --input raw_data/ --output data/filtered_dataset/ --max_tokens 10000000000
```

### Phase 2: Model Training (Using base models provided by the organizers)
```bash
# Method 1: GPU Training (Recommended, faster)
# Note: If encountering CUDA issues, first ensure your GPU environment is set up correctly.
bash train.sh

# Method 2: CPU Training (Verified, slower but stable)
bash train_cpu.sh
# Parameter settings: batch_size=1, max_steps=20, learning_rate=1e-4, lora_r=8

# Training output directory example: results/YYYYMMDD_HHMMSS/dora_model_cpu/
# Contains: adapter_config.json, adapter_model.safetensors, checkpoints, etc.
```

### Phase 3: Model Merging (Required Step)
```bash
# Merge DoRA adapter with the base model to generate standard HuggingFace format
bash scripts/run_merge_dora.sh \
    --model_name_or_path data4elm/Llama-400M-12L \
    --lora_model_path results/YYYYMMDD_HHMMSS/dora_model_cpu \
    --output_model_path results/YYYYMMDD_HHMMSS/merged_model \
    --device cpu

# Output: Complete merged model, including model.safetensors, config.json, etc.
```

### Phase 4: ELMB Evaluation (Obtain final S score)
```bash
cd lm-evaluation-harness

# Step 1: Base model evaluation (Obtain Sbase)
lm_eval --model hf \
    --model_args pretrained=data4elm/Llama-400M-12L,trust_remote_code=True \
    --tasks elmb_roleplay \
    --device cpu \
    --batch_size 1 \
    --log_samples \
    --output_path ../eval_results/baseline

# Step 2: Trained model evaluation (Obtain Simprove)
lm_eval --model hf \
    --model_args pretrained=../results/YYYYMMDD_HHMMSS/merged_model/,trust_remote_code=True \
    --tasks elmb_roleplay \
    --device cpu \
    --batch_size 1 \
    --log_samples \
    --output_path ../eval_results/trained_model

# Step 3: Calculate S score
# S = Simprove - Sbase
# Example results: Sbase(acc=0.8), Simprove(acc=0.6) -> S = -0.2
```

## Detailed Implementation Guide

### Quick Start (Testing Process)
```bash
# 1. Create test dataset
python create_test_dataset.py

# 2. Run CPU training (3-5 minutes to complete)
bash train_cpu.sh

# 3. Merge model
bash scripts/run_merge_dora.sh \
    --model_name_or_path data4elm/Llama-400M-12L \
    --lora_model_path results/$(ls results/ | tail -1)/dora_model_cpu \
    --output_model_path results/$(ls results/ | tail -1)/merged_model \
    --device cpu

# 4. Evaluation comparison
cd lm-evaluation-harness
lm_eval --model hf --model_args pretrained=data4elm/Llama-400M-12L,trust_remote_code=True --tasks elmb_roleplay --device cpu --batch_size 1 --limit 5 --output_path ../eval_results/baseline_test
lm_eval --model hf --model_args pretrained=../results/$(ls ../results/ | tail -1)/merged_model/,trust_remote_code=True --tasks elmb_roleplay --device cpu --batch_size 1 --limit 5 --output_path ../eval_results/trained_test
```

### Production Environment Process (Full Competition)
```bash
# TBD: Complete 10B token data processing flow
# 1. Large-scale data filtering
# 2. GPU training (after resolving CUDA issues)
# 3. Full evaluation for all 4 ELMB tasks
# 4. Hyperparameter optimization and multiple experiments
```

## Key Hyperparameter Requirements (Strictly Adhere)
```yaml
# Training hyperparameters (Cannot be changed)
num_train_epochs: 1
learning_rate: 1e-5  # Note: Use 1e-4 for testing, 1e-5 for official competition
lora_r: 16          # Note: Use 8 for CPU testing, 16 for official competition

# Evaluation hyperparameters (Ensure consistency)
batch_size: 1
device: "cuda:0"    # Use "cuda:0" if CUDA is available, otherwise "cpu"
tasks: "elmb_functioncalling,elmb_chatrag,elmb_reasoning,elmb_roleplay"
```

### Verified Training Configuration
```yaml
# CPU training configuration (Verified working)
block_size: 128
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
lora_r: 8
max_steps: 20  # For testing, use more steps for official training
learning_rate: 1e-4
```

## ELMB Task Specialization Strategies

### 1. Roleplay Task Optimization (Verified)
- **Data filtering focus**: Dialogue quality, role consistency, situational understanding
- **Training data**: High-quality dialogue data, role-playing scenarios
- **Evaluation metric**: acc_norm for elmb_roleplay
- **Test results**: Baseline acc=0.8, Trained acc=0.6 (Data quality needs optimization)

### 2. Reasoning Task Optimization (To be verified)
- **Data filtering focus**: Logical chain completeness, clarity of reasoning steps
- **Training data**: Mathematical reasoning, logical reasoning, causal relationship data
- **Evaluation metric**: acc_norm for elmb_reasoning
- **Evaluation command**: `--tasks elmb_reasoning`

### 3. Function Calling Task Optimization (To be verified)
- **Data filtering focus**: API call accuracy, parameter correctness, tool usage
- **Training data**: API documentation, function call examples, tool usage guides
- **Evaluation metric**: acc_norm for elmb_functioncalling
- **Evaluation command**: `--tasks elmb_functioncalling`

### 4. RAG Task Optimization (To be verified)
- **Data filtering focus**: Retrieval relevance, answer accuracy, citation quality
- **Training data**: Question-answer pairs, document summaries, knowledge base content
- **Evaluation metric**: acc_norm for elmb_chatrag
- **Evaluation command**: `--tasks elmb_chatrag`

## Submission Checklist

### Required Submission Materials
1. **Trained Model Checkpoint**
   ```bash
   ./trained_model/
   ├── adapter_config.json      # DoRA configuration
   ├── adapter_model.safetensors # DoRA weights
   ├── README.md               # Model description
   ├── training_args.bin       # Training parameters
   └── merged_model/           # Merged complete model
       ├── config.json
       ├── model.safetensors
       └── tokenizer files
   ```

2. **Filtered Dataset**
   ```bash
   ./filtered_data/
   ├── train.jsonl            # Training data (<10B tokens)
   ├── data_stats.json        # Data statistics
   └── filter_config.json     # Filtering algorithm configuration
   ```

3. **Filtering and Training Code**
   ```bash
   ./code/
   ├── create_test_dataset.py  # Test data generation (Existing)
   ├── data_filter.py         # Data filtering algorithm (TBD)
   ├── train_cpu.sh           # CPU training script (Verified)
   ├── train.sh               # GPU training script (Existing)
   ├── scripts/run_merge_dora.sh # Model merging (Verified)
   ├── requirements.txt       # Dependency list
   └── run_pipeline.sh        # Complete pipeline script
   ```

4. **ELMB Evaluation Results**
   ```bash
   ./eval_results/
   ├── baseline/              # Base model results
   ├── trained_model/         # Trained model results
   ├── results_summary.json   # S score summary
   └── final_score_report.json # Final score report
   ```

### Final Verification Process
```bash
# Complete verification process
bash run_complete_pipeline.sh

# Steps include:
# 1. Data filtering verification
# 2. Model training verification
# 3. Model merging verification
# 4. ELMB evaluation verification
# 5. S score calculation verification
```

## Competition Strategy Suggestions

### Grand Prize Strategy (Maximize total score)
- Balance data allocation across four tasks (25% each)
- Develop versatile filtering techniques
- Focus on optimizing performance for weaker tasks

### Single Task Prize Strategy (Specialize in a specific task)
- Invest 80% of data budget into the target task
- Develop task-specific filtering algorithms
- Deeply optimize evaluation metrics for a single task

### Innovation Prize Strategy (Technical Innovation)
- Develop non-traditional filtering methods (e.g., active learning, reinforcement learning)
- Implement explainable filtering decision mechanisms
- Explore multi-modal data filtering techniques

## Time Planning Suggestions
- **May-June**: Data analysis, filtering technique development and verification
- **Early July**: Large-scale data filtering and quality checks
- **July-August**: Model training, hyperparameter tuning, and performance verification
- **Late August**: Final submission material preparation and completeness checks

## Common Issues Troubleshooting

### Training Related
- **OOM Error**: Reduce batch_size, increase gradient_accumulation_steps
- **Convergence Issues**: Check learning rate settings, verify data quality
- **Save Error**: Confirm output directory permissions and disk space
- **CUDA Issues**: Use train_cpu.sh as an alternative

### Evaluation Related
- **Task Failure**: Run failed tasks individually, check dataset availability
- **Abnormal Results**: Verify model path, prompt format matching
- **Performance Degradation**: Check baseline result source, confirm evaluation consistency
- **DoRA adapter loading failed**: Must merge DoRA adapter to base model first

### Solved Issues (Experience Summary)
1. **Training Stuck**: Use CPU training script, reduce parameter scale
2. **DoRA Evaluation Failed**: Must merge DoRA adapter to base model first
3. **Dataset Path Error**: Confirm train.sh dataset_path setting
4. **wandb Blocked**: Add `--report_to none` parameter

By following this complete workflow, you can start from data filtering, complete model training and evaluation step by step, and finally obtain a submitable S score result.
