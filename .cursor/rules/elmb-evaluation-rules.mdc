---
description: 
globs: 
alwaysApply: true
---


description: ELMB Evaluation Rules & Framework Guideglobs:alwaysApply: true

ELMB Evaluation Rules & Framework

Purpose

Standardize model evaluation for the Edge Language Model Benchmark (ELMB) using lm-evaluation-harness. Focus on measuring data filtering effectiveness and maximizing S = Simprove - Sbase across four tasks.

Core Evaluation Principles
	•	Evaluate with ELMB to measure filtered data effectiveness.
	•	S = Simprove - Sbase; maximize improvement.
	•	Four tasks: Roleplay, Reasoning, Function Calling, RAG—each scored independently.

Task Configurations

Roleplay (‎⁠elmb_roleplay⁠)
	•	Task File: ‎⁠lm-evaluation-harness/lm_eval/tasks/elmb_roleplay/elmb_roleplay.yaml⁠
	•	Dataset: ‎⁠data4elm/ELMB-RolePlay⁠
	•	Metrics: ‎⁠acc⁠, ‎⁠acc_norm⁠
	•	Output: multiple_choice
	•	Prompt: ‎⁠Question: {{question}}\nAnswer:⁠

Reasoning (‎⁠elmb_reasoning⁠)
	•	Task File: ‎⁠lm-evaluation-harness/lm_eval/tasks/elmb_reasoning/elmb_reasoning.yaml⁠
	•	Dataset: ‎⁠data4elm/ELMB-Reasoning⁠
	•	Metrics: ‎⁠acc⁠, ‎⁠acc_norm⁠
	•	Output: multiple_choice
	•	Prompt: ‎⁠Question: {{question}}\nAnswer:⁠

Function Calling (‎⁠elmb_functioncalling⁠)
	•	Task File: ‎⁠lm-evaluation-harness/lm_eval/tasks/elmb_functioncalling/elmb_functioncalling.yaml⁠
	•	Dataset: ‎⁠data4elm/ELMB-FunctionCalling⁠
	•	Metrics: ‎⁠acc⁠, ‎⁠acc_norm⁠
	•	Output: multiple_choice
	•	Prompt: ‎⁠Question: {{question}}\nAnswer:⁠

RAG (‎⁠elmb_chatrag⁠)
	•	Task File: ‎⁠lm-evaluation-harness/lm_eval/tasks/elmb_chatrag/elmb_chatrag.yaml⁠
	•	Dataset: ‎⁠data4elm/ELMB-ChatRAG⁠
	•	Metrics: ‎⁠acc⁠, ‎⁠acc_norm⁠
	•	Output: multiple_choice
	•	Prompt: ‎⁠Context: {{ctx}}\nQuestion: {{question}}\nAnswer:⁠

Standard Evaluation Commandlm_eval --model hf \
    --model_args pretrained=[MODEL_PATH],trust_remote_code=True,cache_dir=~/.cache \
    --tasks elmb_functioncalling,elmb_chatrag,elmb_reasoning,elmb_roleplay \
    --device cuda:0 \
    --batch_size 1 \
    --log_samples \
    --output_path ./eval_results/[EXPERIMENT_NAME]

Best Practices
	•	Use identical hardware/software environments and fixed random seeds for reproducibility.
	•	Monitor GPU memory; batch_size=1 recommended.
	•	Save intermediate results to avoid interruptions.
	•	Evaluate all four tasks; ensure results are complete and in the correct range.
	•	For troubleshooting: check model/data paths, reduce batch size for OOM errors, validate prompt formats.

Reporting
	•	Summarize results for all tasks.
	•	Report both ‎⁠acc⁠ and ‎⁠acc_norm⁠, prioritizing ‎⁠acc_norm⁠.
	•	Calculate and report S for each task and overall.
	•	Include error analysis and improvement suggestions.

Analogy: Treat evaluation like a standardized exam—same instructions, same environment, and fair scoring for all candidates.