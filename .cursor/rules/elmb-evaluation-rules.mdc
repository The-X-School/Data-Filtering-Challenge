---
description: 
globs: 
alwaysApply: true
---
# ELMB Evaluation Rules and Framework Guidance

## Core Understanding of ELMB Benchmark Evaluation
1. **Evaluation Goal**: Measure the effectiveness of data filtering techniques through ELMB (Edge Language Model Benchmark)
2. **Core Metric**: S = Simprove - Sbase (performance improvement of filtered dataset)
3. **Evaluation Tasks**: 4 core tasks, each independently evaluated and contributing to the total score
4. **Evaluation Framework**: Standardized evaluation process based on lm-evaluation-harness

## Configuration of Four Core ELMB Tasks

### 1. Roleplay Task (elmb_roleplay)
- **Task File**: `lm-evaluation-harness/lm_eval/tasks/elmb_roleplay/elmb_roleplay.yaml`
- **Dataset**: `data4elm/ELMB-RolePlay`
- **Evaluation Metrics**: `acc` (accuracy) and `acc_norm` (normalized accuracy)
- **Output Type**: multiple_choice
- **Prompt Format**: "Question: {{question}}\nAnswer:"
- **Evaluation Focus**: Role-playing ability in interactive digital environments

### 2. Reasoning Task (elmb_reasoning)
- **Task File**: `lm-evaluation-harness/lm_eval/tasks/elmb_reasoning/elmb_reasoning.yaml`
- **Dataset**: `data4elm/ELMB-Reasoning`  
- **Evaluation Metrics**: `acc` and `acc_norm`
- **Output Type**: multiple_choice
- **Prompt Format**: "Question: {{question}}\nAnswer:"
- **Evaluation Focus**: Complex problem-solving and logical reasoning ability (robotics applications)

### 3. Function Calling Task (elmb_functioncalling)
- **Task File**: `lm-evaluation-harness/lm_eval/tasks/elmb_functioncalling/elmb_functioncalling.yaml`
- **Dataset**: `data4elm/ELMB-FunctionCalling`
- **Evaluation Metrics**: `acc` and `acc_norm`
- **Output Type**: multiple_choice
- **Prompt Format**: "Question: {{question}}\nAnswer:"
- **Evaluation Focus**: Mobile device interaction and API calling capabilities

### 4. RAG Task (elmb_chatrag)
- **Task File**: `lm-evaluation-harness/lm_eval/tasks/elmb_chatrag/elmb_chatrag.yaml`
- **Dataset**: `data4elm/ELMB-ChatRAG`
- **Evaluation Metrics**: `acc` and `acc_norm`
- **Output Type**: multiple_choice
- **Prompt Format**: "Context: {{ctx}}\nQuestion: {{question}}\nAnswer:"
- **Evaluation Focus**: Retrieval-augmented generation and document understanding capabilities

## Standard Evaluation Commands and Processes

### Basic Evaluation Command
```bash
lm_eval --model hf \
    --model_args pretrained=[MODEL_PATH],trust_remote_code=True,cache_dir=~/.cache \
    --tasks elmb_functioncalling,elmb_chatrag,elmb_reasoning,elmb_roleplay \
    --device cuda:0 \
    --batch_size 1 \
    --log_samples \
    --output_path ./eval_results/[EXPERIMENT_NAME]
```

### Evaluation Parameter Explanation
- `--model hf`: Use HuggingFace model loader
- `--model_args`: Model configuration parameters
  - `pretrained`: Model path (local or HF Hub)
  - `trust_remote_code=True`: Allow execution of remote code
  - `cache_dir`: Model cache directory
- `--tasks`: Specify ELMB four tasks (comma-separated)
- `--device`: Specify computing device (cuda:0, cuda:1, etc.)
- `--batch_size`: Batch processing size (recommended as 1 for stability)
- `--log_samples`: Record specific sample prediction results
- `--output_path`: Result output path

## Evaluation Result Interpretation Rules

### 1. Performance Metric Understanding
- **acc (Accuracy)**: Accuracy rate, directly reflects the proportion of correct model answers
- **acc_norm (Normalized Accuracy)**: Normalized accuracy rate, considers selection bias correction accuracy
- **Evaluation Priority**: Prioritize `acc_norm` as the main evaluation metric

### 2. Baseline Performance (Sbase) Acquisition
- **Definition**: Performance of models pre-trained on baseline datasets (e.g., Fineweb) on ELMB
- **Acquisition Method**: Provided by competition organizers with baseline models and performance data
- **Recording Format**: Each task independently records Sbase values

### 3. Improved Performance (Simprove) Calculation
- **Definition**: Performance of models after continued pre-training with filtered datasets on ELMB
- **Calculation Method**: Run standard evaluation process under identical evaluation settings
- **Recording Requirements**: Ensure complete consistency with Sbase evaluation conditions

### 4. Performance Improvement (S) Calculation
- **Single Task Improvement**: S_task = Simprove_task - Sbase_task
- **Overall Improvement**: S_total = S_roleplay + S_reasoning + S_functioncalling + S_rag
- **Winning Criteria**: Maximize S_total for grand prize, maximize individual S_task for single task awards

## Evaluation Best Practices

### 1. Evaluation Environment Consistency
- **Hardware Configuration**: Ensure evaluation uses identical GPU models and memory configurations
- **Software Environment**: Use identical versions of lm-evaluation-harness and dependency packages
- **Random Seeds**: Set fixed random seeds to ensure result reproducibility

### 2. Batch Evaluation Management
- **Resource Monitoring**: Monitor GPU memory usage to avoid OOM errors
- **Result Backup**: Timely save intermediate results to prevent unexpected interruptions
- **Multiple Runs**: Conduct multiple evaluations and take averages to improve result reliability

### 3. Result Validation Checks
- **Data Integrity**: Confirm all four tasks have complete results
- **Metric Reasonableness**: Check if accuracy rates are within reasonable ranges (0-1)
- **Comparative Validation**: Cross-validate with known benchmarks

## Evaluation Debugging and Troubleshooting

### 1. Common Error Handling
- **CUDA Memory Insufficient**: Reduce batch_size or use gradient checkpointing
- **Model Loading Failed**: Check model path and permission settings
- **Dataset Loading Errors**: Verify dataset path and format correctness

### 2. Performance Anomaly Diagnosis
- **Abnormally Low Accuracy**: Check if model loaded correctly and prompt format matches
- **Task Failures**: Run failed tasks individually for detailed diagnosis
- **Result Instability**: Increase evaluation rounds and check randomness sources

### 3. Evaluation Result Analysis
- **Single Task Analysis**: Separately analyze performance and improvement potential of four tasks
- **Error Case Analysis**: Review log_samples output to analyze specific error patterns
- **Performance Bottleneck Identification**: Determine which tasks are main limiting factors

## Automated Evaluation Process

### 1. Batch Model Evaluation Script
```bash
#!/bin/bash
MODELS=("model1" "model2" "model3")
for MODEL in "${MODELS[@]}"; do
    lm_eval --model hf \
        --model_args pretrained=$MODEL,trust_remote_code=True \
        --tasks elmb_functioncalling,elmb_chatrag,elmb_reasoning,elmb_roleplay \
        --device cuda:0 \
        --batch_size 1 \
        --log_samples \
        --output_path ./eval_results/${MODEL}_elmb
done
```

### 2. Result Aggregation and Comparison
- **Result Collection**: Automatically collect evaluation results from all models
- **Performance Comparison**: Generate performance comparison tables and visualization charts
- **Improvement Calculation**: Automatically calculate performance improvements of each model relative to baseline

## Evaluation Report Generation

### 1. Standard Report Format
- **Model Basic Information**: Model name, training data, parameter count, etc.
- **Evaluation Result Summary**: Detailed performance data for four tasks
- **Performance Improvement Analysis**: Degree of improvement relative to baseline and rankings
- **Error Analysis**: Main error types and improvement suggestions

### 2. Visualization Display
- **Performance Radar Chart**: Comprehensive performance across four task dimensions
- **Improvement Bar Chart**: Performance improvement comparison across tasks
- **Trend Analysis Chart**: Effect trends of different data filtering strategies

## Competition Submission Preparation

### 1. Final Evaluation Checklist
- [ ] All four ELMB tasks have been completely evaluated
- [ ] Evaluation result format meets requirements
- [ ] Model checkpoints are complete and loadable
- [ ] Evaluation environment and parameter records are complete
- [ ] Results are reproducible

### 2. Submission Material Preparation
- **Model Checkpoints**: Completed trained model files
- **Evaluation Results**: Detailed ELMB evaluation reports
- **Evaluation Scripts**: Complete scripts for reproducing evaluation results
- **Environment Configuration**: Dependency package versions and runtime environment descriptions
