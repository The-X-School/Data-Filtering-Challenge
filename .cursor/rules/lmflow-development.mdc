---
description: 
globs: 
alwaysApply: true
---

LMFlow Development Workflow & Coding Standards

Technology Stack
	1.	Core Dependencies:
	▫	PyTorch: >= 2.0.1
	▫	HuggingFace Transformers: >= 4.31.0
	▫	DeepSpeed: >= 0.14.4
	▫	PEFT: = 0.15.2
	▫	Accelerate: >= 0.27.2
	2.	Code Quality Tools:
	▫	Ruff: Linting and formatting, targeting Python 3.9
	▫	Config: Double quotes, 4-space indentation, docstring formatting supported

Development Environment Setup
	1.	Requirements:
	▫	Linux OS (Ubuntu 20.04 tested)
	▫	Python 3.9
	▫	CUDA-enabled GPU (recommended)
	▫	MPI support (‎⁠conda install mpi4py⁠)
	2.	Installation Steps:git clone -b v0.0.9 https://github.com/OptimalScale/LMFlow.git
cd LMFlow
git checkout data4elm
conda create -n lmflow python=3.9 -y
conda activate lmflow
conda install mpi4py
pip install -e .

Contribution Standards
	1.	Module Development:
	▫	All new features must inherit from the appropriate base class
	▫	Use Factory Pattern for component selection
	▫	Maintain interface consistency and backward compatibility
	2.	File Organization:
	▫	Models: ‎⁠src/lmflow/models/⁠
	▫	Data processing: ‎⁠src/lmflow/datasets/⁠
	▫	Training pipelines: ‎⁠src/lmflow/pipeline/⁠
	▫	Utilities: ‎⁠src/lmflow/utils/⁠
	3.	Naming Conventions:
	▫	Files: lowercase with underscores, e.g., ‎⁠hf_decoder_model.py⁠
	▫	Classes: CamelCase, e.g., ‎⁠HfDecoderModel⁠
	▫	Functions: lowercase with underscores, e.g., ‎⁠load_model⁠
	▫	Constants: UPPERCASE_WITH_UNDERSCORES, e.g., ‎⁠DEFAULT_CONFIG⁠

Configuration Management
	1.	Training Configs:
	▫	DeepSpeed: ‎⁠configs/ds_config_*.json⁠
	▫	Accelerate: ‎⁠configs/accelerate_*.yaml⁠
	▫	Iterative training: ‎⁠configs/iterative_*.yaml⁠
	2.	Naming:
	▫	Clearly indicate strategy and purpose, e.g., ‎⁠ds_config_zero3_for_eval.json⁠
	▫	Distinguish hardware configs, e.g., ‎⁠accelerator_multigpu_config.yaml⁠

Example Scripts
	1.	‎⁠examples/⁠ Directory:
	▫	Each major function has a dedicated script
	▫	Scripts must include full argument parsing and error handling
	▫	Provide clear usage and parameter documentation
	2.	Common Examples:
	▫	‎⁠finetune.py⁠: Standard fine-tuning
	▫	‎⁠inference.py⁠: Model inference
	▫	‎⁠evaluation.py⁠: Model evaluation
	▫	‎⁠*_gradio.py⁠: Web UI examples

Testing & Validation
	1.	Code Testing:
	▫	New features require corresponding test cases
	▫	Use small datasets for functional validation
	▫	Ensure compatibility with existing features
	2.	Performance Testing:
	▫	Test on various hardware setups
	▫	Validate memory usage and training speed
	▫	Check distributed training correctness

Documentation & Comments
	1.	Code Comments:
	▫	All comments in English
	▫	Important functions require detailed docstrings
	▫	Inline comments for complex logic
	2.	Config Documentation:
	▫	Each config file must include parameter explanations
	▫	Provide usage scenarios and applicable conditions
	▫	Note performance impact and resource requirements

Version Compatibility
	1.	Dependency Management:
	▫	Specify version ranges for key dependencies
	▫	Regularly test compatibility with latest versions
	▫	Update ‎⁠requirements.txt⁠ as needed
	2.	Backward Compatibility:
	▫	New features must not break existing APIs
	▫	Provide migration guides for deprecated features
	▫	Maintain config file format compatibility

Performance Optimization
	1.	Memory:
	▫	Use appropriate data types (bf16, fp16)
	▫	Implement gradient checkpointing
	▫	Optimize data loading and preprocessing
	2.	Computation:
	▫	Support multiple distributed training strategies
	▫	Implement efficient attention mechanisms
	▫	Optimize forward and backward passes

Error Handling
	1.	Exception Handling:
	▫	Use explicit exception types
	▫	Provide informative error messages
	▫	Add recovery mechanisms at critical points
	2.	Logging:
	▫	Use a unified logging format
	▫	Log key operations and parameters
	▫	Distinguish log levels appropriately