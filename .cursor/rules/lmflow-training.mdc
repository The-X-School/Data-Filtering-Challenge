---
description: 
globs: 
alwaysApply: true
---


description: LMFlow training and fine-tuning best practicesglobs: [“train.sh”, “examples/finetune*.py”, “examples/dpo*.py”, “examples/raft*.py”, “configs/ds_config*.json”]alwaysApply: true

LMFlow Training & Fine-Tuning Best Practices

Fine-Tuning Strategies
	1.	DoRA (Weight-Decomposed Low-Rank Adaptation):
	▫	Primary fine-tuning method; more efficient than LoRA.
	▫	Fixed hyperparameters: ‎⁠learning_rate=1e-5⁠, ‎⁠lora_r=16⁠, ‎⁠num_train_epochs=1⁠.
	▫	Enable with ‎⁠--use_dora 1⁠ and specify target layers with ‎⁠--lora_target_modules⁠.
	2.	Other Supported Methods:
	▫	DPO (Direct Preference Optimization): For human preference alignment.
	▫	Iterative DPO: Iterative preference optimization.
	▫	RAFT: For retrieval-augmented generation alignment.
	▫	Reward Modeling: For training reward models.

Training Configuration Management
	1.	DeepSpeed Configurations:
	▫	Choose ZeRO strategy based on GPU memory:
	⁃	‎⁠ds_config_zero0_no_offload.json⁠: Small models, multi-GPU.
	⁃	‎⁠ds_config_zero2.json⁠: Medium models, parameter sharding.
	⁃	‎⁠ds_config_zero3.json⁠: Large models, full sharding.
	2.	Fixed Hyperparameters:
	▫	To eliminate confounding factors, always use:
	⁃	‎⁠num_train_epochs = 1⁠
	⁃	‎⁠learning_rate = 1e-5⁠
	⁃	‎⁠lora_r = 16⁠
	3.	Data Processing:
	▫	Support up to 10B tokens for training.
	▫	Use ‎⁠--block_size 1024⁠ for sequence length.
	▫	Optimize preprocessing with ‎⁠--preprocessing_num_workers⁠.

Training Script Usage
	1.	Standard Training Workflow:bash train.sh \
  --model_name_or_path [MODEL_PATH] \
  --dataset_path [DATASET_PATH] \
  --output_dora_path [OUTPUT_PATH]

	2.	Key Parameter Settings:
	▫	‎⁠--per_device_train_batch_size⁠: Adjust based on GPU memory.
	▫	‎⁠--gradient_accumulation_steps⁠: Effectively increase batch size.
	▫	‎⁠--save_steps⁠: Checkpoint save frequency.
	▫	‎⁠--logging_steps⁠: Logging frequency.
	3.	Resuming Training:
	▫	Use ‎⁠--resume_from_checkpoint [CHECKPOINT_PATH]⁠.
	▫	Checkpoint path format: ‎⁠[model-dir]/checkpoint-[index]⁠.

Validation & Evaluation
	1.	During Training:
	▫	‎⁠--validation_split_percentage 5⁠: Use 5% of data for validation.
	▫	‎⁠--eval_strategy steps⁠: Evaluate by steps.
	▫	‎⁠--eval_steps 20⁠: Evaluate every 20 steps.
	2.	ELMB Benchmark Testing:
	▫	Use ‎⁠lm-evaluation-harness⁠ for evaluation.
	▫	Tasks: ‎⁠elmb_roleplay⁠, ‎⁠elmb_reasoning⁠, ‎⁠elmb_functioncalling⁠, ‎⁠elmb_chatrag⁠.
	▫	Merge DoRA weights into the base model before evaluation.

Model Management
	1.	Merging Weights:bash ./scripts/run_merge_dora.sh \
  --model_name_or_path [BASE_MODEL] \
  --lora_model_path [DORA_PATH] \
  --output_model_path [MERGED_PATH]

	2.	Model Upload:
	▫	See ‎⁠example_upload_peft_model.py⁠ for reference.
	▫	Upload to HuggingFace Hub for sharing.

Performance Optimization Tips
	1.	Memory Optimization:
	▫	Use appropriate ZeRO strategy.
	▫	Enable gradient checkpointing.
	▫	Adjust batch size and accumulation steps.
	2.	Speed Optimization:
	▫	Use ‎⁠bf16⁠ mixed precision training.
	▫	Adjust ‎⁠dataloader_num_workers⁠.
	▫	Optimize ‎⁠preprocessing_num_workers⁠.
	3.	Monitoring & Debugging:
	▫	Use WandB for training monitoring.
	▫	Check training logs in the ‎⁠log/⁠ directory.
	▫	Monitor memory and GPU utilization.