---
description: 
globs: 
alwaysApply: true
---


description: LMFlow troubleshooting and debugging guideglobs: [”/*.py”, “/.sh”, “**/.json”, “/*.yaml”, “log//*”]alwaysApply: true

LMFlow Troubleshooting & Debugging Guide

Common Training Issues
	1.	Out of Memory (OOM):
	▫	Symptoms: CUDA out of memory errors.
	▫	Solutions:
	⁃	Lower ‎⁠per_device_train_batch_size⁠
	⁃	Increase ‎⁠gradient_accumulation_steps⁠
	⁃	Use a more advanced ZeRO stage (e.g., ZeRO-2 → ZeRO-3)
	⁃	Enable CPU offloading
	2.	Distributed Training Failures:
	▫	Symptoms: Inter-process communication errors or hangs.
	▫	Solutions:
	⁃	Check if ‎⁠--master_port⁠ is occupied
	⁃	Ensure all nodes have identical environments
	⁃	Increase ‎⁠--ddp_timeout⁠
	⁃	Verify network/firewall settings
	3.	Slow Data Loading:
	▫	Symptoms: Low GPU utilization during training.
	▫	Solutions:
	⁃	Adjust ‎⁠--dataloader_num_workers⁠
	⁃	Optimize ‎⁠--preprocessing_num_workers⁠
	⁃	Use faster storage devices
	⁃	Preprocess and cache data

Configuration Issues
	1.	DeepSpeed Config Errors:
	▫	Symptoms: DeepSpeed initialization failures.
	▫	Checklist:
	⁃	Validate JSON config syntax
	⁃	Match ZeRO stage to model size
	⁃	Check optimizer and scheduler settings
	⁃	Ensure batch size consistency
	2.	Model Loading Failures:
	▫	Symptoms: Model file not found or weight mismatch.
	▫	Solutions:
	⁃	Verify model path
	⁃	Check ‎⁠trust_remote_code⁠ setting
	⁃	Ensure model/tokenizer compatibility
	⁃	Check cache directory permissions

Evaluation Issues
	1.	ELMB Evaluation Failures:
	▫	Symptoms: Errors during ‎⁠lm_eval⁠ execution.
	▫	Solutions:
	⁃	Confirm ‎⁠lm-evaluation-harness⁠ is installed
	⁃	Check model path and merged weights
	⁃	Verify CUDA device availability
	⁃	Ensure sufficient memory for evaluation
	2.	Abnormal Evaluation Results:
	▫	Symptoms: Scores are too low or unreasonable.
	▫	Checklist:
	⁃	Use the correct merged model
	⁃	Validate tokenizer settings
	⁃	Check data preprocessing
	⁃	Compare with baseline results

Environment & Dependency Issues
	1.	CUDA Version Incompatibility:
	▫	Symptoms: CUDA errors or performance issues.
	▫	Solutions:
	⁃	Match PyTorch and CUDA versions
	⁃	Reinstall the correct PyTorch version
	⁃	Verify GPU driver version
	⁃	Check CUDA toolkit installation
	2.	Dependency Version Conflicts:
	▫	Symptoms: Import errors or abnormal behavior.
	▫	Solutions:
	⁃	Install dependencies strictly from ‎⁠requirements.txt⁠
	⁃	Use a dedicated virtual environment
	⁃	Check compatibility of key dependencies
	⁃	Clear old cache files

Debugging Tools & Methods
	1.	Log Analysis:
	▫	Training logs: Check files in ‎⁠log/⁠ directory
	▫	Error logs: Review ‎⁠.err⁠ files for details
	▫	WandB monitoring: Observe loss curves and metrics
	▫	System monitoring: Use ‎⁠htop⁠, ‎⁠nvidia-smi⁠ for resource usage
	2.	Stepwise Debugging:
	▫	Test with small datasets
	▫	Validate on a single GPU first
	▫	Reduce model or batch size for testing
	▫	Print key variables and states

Performance Optimization
	1.	Training Speed:
	▫	Choose appropriate precision (bf16/fp16)
	▫	Adjust data/model parallelism strategies
	▫	Optimize data preprocessing and loading
	▫	Use gradient checkpointing to balance memory and speed
	2.	Memory Usage:
	▫	Select suitable ZeRO stage
	▫	Enable activation checkpointing
	▫	Adjust batch size and sequence length
	▫	Consider CPU offloading

Common Error Codes
	1.	Error 137: Killed due to insufficient memory
	▫	Solution: Reduce memory usage or increase system memory
	2.	Error 1: General error
	▫	Solution: Check detailed error logs, usually config or data issues
	3.	Error 2: File not found
	▫	Solution: Verify all file paths

Emergency Handling
	1.	Training Interruptions:
	▫	Check latest checkpoint
	▫	Resume with ‎⁠--resume_from_checkpoint⁠
	▫	Validate resumed training state
	▫	Monitor for data/state anomalies
	2.	Abnormal Results:
	▫	Compare results across checkpoints
	▫	Rerun key steps for validation
	▫	Check random seeds and data order
	▫	Compare with baseline results

Community Support Channels
	1.	Official:
	▫	GitHub Issues: For technical problems and bug reports
	▫	Discord: For real-time discussion and quick help
	▫	Documentation: For the latest usage guides
	2.	Debug Info Collection:
	▫	System environment (OS, CUDA, Python version)
	▫	Full error logs
	▫	Config files used
	▫	Minimal reproducible example