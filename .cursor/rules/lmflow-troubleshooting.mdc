---
description: 
globs: 
alwaysApply: true
---
---
description: "LMFlow troubleshooting and debugging guide"
globs: ["**/*.py", "**/*.sh", "**/*.json", "**/*.yaml", "log/**/*"]
always: true
---

# LMFlow Troubleshooting and Debugging Guide

## Common Training Issues
1. **Out of Memory (OOM)**:
   - **Symptom**: CUDA out of memory error
   - **Solution**: 
     - Reduce `per_device_train_batch_size`
     - Increase `gradient_accumulation_steps`
     - Use more advanced ZeRO strategies (ZeRO-2 â†’ ZeRO-3)
     - Enable CPU offloading

2. **Distributed Training Failure**:
   - **Symptom**: Inter-process communication errors or hangs
   - **Solution**:
     - Check if `--master_port` is occupied
     - Confirm consistent environment across all nodes
     - Increase `--ddp_timeout` value
     - Verify network connectivity and firewall settings

3. **Slow Data Loading**:
   - **Symptom**: Low GPU utilization during training
   - **Solution**:
     - Adjust `--dataloader_num_workers`
     - Optimize `--preprocessing_num_workers`
     - Use faster storage devices
     - Pre-process and cache data

## Configuration Related Issues
1. **DeepSpeed Configuration Errors**:
   - **Symptom**: DeepSpeed initialization failure
   - **Checkpoints**:
     - Confirm JSON configuration file syntax is correct
     - Verify ZeRO stage matches model size
     - Check optimizer and scheduler configurations
     - Ensure batch size settings are consistent

2. **Model Loading Failed**:
   - **Symptom**: Model file not found or weights mismatch
   - **Solution**:
     - Check if the model path is correct
     - Verify `trust_remote_code` setting
     - Confirm model and tokenizer compatibility
     - Check cache directory permissions

## Evaluation Related Issues
1. **ELMB Evaluation Failure**:
   - **Symptom**: lm_eval execution error
   - **Solution**:
     - Confirm lm-evaluation-harness is correctly installed
     - Check if model path and weights have been merged
     - Verify CUDA device availability
     - Ensure sufficient memory to run evaluation

2. **Abnormal Evaluation Results**:
   - **Symptom**: Score is too low or unreasonable
   - **Checkpoints**:
     - Confirm the correct merged model was used
     - Verify tokenizer settings
     - Check if data preprocessing is correct
     - Compare with baseline model results

## Environment and Dependency Issues
1. **CUDA Version Incompatibility**:
   - **Symptom**: CUDA-related errors or performance issues
   - **Solution**:
     - Check PyTorch and CUDA version matching
     - Reinstall the corresponding PyTorch version
     - Verify GPU driver version
     - Check CUDA toolkit installation

2. **Dependency Version Conflicts**:
   - **Symptom**: Import errors or functional anomalies
   - **Solution**:
     - Strictly follow requirements.txt for installation
     - Use isolated virtual environments
     - Check key dependency version compatibility
     - Clear old version cache files

## Debugging Tools and Methods
1. **Log Analysis**:
   - **Training Logs**: Check files in the `log/` directory
   - **Error Logs**: View detailed error information in `.err` files
   - **WandB Monitoring**: Observe loss curves and performance metrics
   - **System Monitoring**: Use `htop`, `nvidia-smi` to monitor resources

2. **Step-by-step Debugging**:
   - **Small Dataset Testing**: Use a small amount of data to verify the process
   - **Single GPU Training**: First verify the configuration on a single GPU
   - **Parameter Reduction**: Reduce model or batch size for testing
   - **Intermediate Output**: Print key variables and states

## Performance Optimization Suggestions
1. **Training Speed Optimization**:
   - Select appropriate precision mode (bf16/fp16)
   - Adjust data parallelism and model parallelism strategies
   - Optimize data preprocessing and loading processes
   - Use gradient checkpointing to balance memory and speed

2. **Memory Usage Optimization**:
   - Use appropriate ZeRO stages
   - Enable activation checkpointing
   - Adjust batch size and sequence length
   - Consider using CPU offloading

## Common Error Codes
1. **Error Code 137**: Out of memory, killed by system
   - Solution: Reduce memory usage or increase system memory

2. **Error Code 1**: General error
   - Solution: Check detailed error logs, usually a configuration or data issue

3. **Error Code 2**: File not found
   - Solution: Check if all file paths are correct

## Emergency Handling Process
1. **Training Interruption Handling**:
   - Check the latest checkpoint
   - Use `--resume_from_checkpoint` to resume
   - Verify training status after resumption
   - Monitor for data or status anomalies

2. **Abnormal Result Handling**:
   - Compare results from multiple checkpoints
   - Rerun critical steps for verification
   - Check random seed and data order
   - Compare with baseline results

## Community Support Channels
1. **Official Channels**:
   - GitHub Issues: Technical questions and bug reports
   - Discord: Real-time discussions and quick help
   - Documentation: Refer to the latest user guide

2. **Debugging Information Collection**:
   - System environment information (OS, CUDA, Python version)
   - Complete error logs
   - Used configuration files
   - Minimal example to reproduce the issue
